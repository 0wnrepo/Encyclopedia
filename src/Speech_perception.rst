.. _direct realism: Direct_realism.html
.. _perception: Perception.html
.. _speech: Speech.html

================================================================================
Speech Perception
================================================================================

**Speech perception** is the `perception`_ of `speech`_; the process by which
the sounds of language are heard, interpreted, and understood.

The study of `speech perception`_ tries to understand how human listeners
recognize speech sounds and use this information in spoken language.

- Infant speech perception

- Cross-language speech perception
  
- Second-language speech perception

- Speech perception in language or hearing impairment

- Noise
  
- Music-language connection

Sonagraphic studies attempt to segment speech into invariant acoustic features.

.. contents::
   :depth: 2

Introduction
================================================================================

Relatively simple speech sounds vary wildly between speakers.

Research
================================================================================

Research has given rise to three main theoretical perspective on `speech
perception`_ that frame much of the empirical work.

1. `Motor theory`_
  
2. `Direct realist theory`_
  
3. `General auditory framework`_

Motor theory
--------------------------------------------------------------------------------

**Motor theory (MT)** is the theory that ...

`Motor theory`_ was developed to explain perceptual nonlinearities.

Claims:

1. The objects of speech are articulatory events rather than acoustic or
   auditory events.
   
2. Speech perception cannot be ascribed to general mechanisms of auditory and
   perceptual learning
   
3. Speech perception depends on a specialized decoder or module that is speech
   specific, unique to humans, and innately organized and part of the large
   biological specialization for the language.

4. MT is parsimonious inasmuch as the same mechanism is used for both speech
   production and speech perception.

Motor theory has undergone significant changes since its initial formulation,
but each version has held Claim 1. 

Direct realist theory
--------------------------------------------------------------------------------

**Direct realist theory (DRT)** is the theory that ...

Direct realist theory is part of a more general theory of `direct realism`_.
   
Claims:

1. Speech is viewed as an environmental event in which the speaker's phonetic
   gesture structure the acoustic array; listeners perceive the speech events
   from the information about them in the proximal acoustic signal.

2. Speech perception is not restricted to humans.
   
3. The perception of distal environmental events is not unique to speech

1. The objects of speech are articulatory events rather than acoustic or
   auditory events (like MT) 2. The articulatory objects of perception are
   actual, phonetically structured vocal tract movements, or gestures 3. The
   articulatory objects of perception are not events that are causally
   antecedent to these movements, such as neuromotor commands or intended
   gestures 4. Specializing mechanisms do not play a role in speech percetion

Claim: There is typically a lack of correspondence between acoustic cue and
perceived phoneme, and in all these cases it appears that perception mirrors
articulation more closely than sound.

Claim: Perceivers utilize articulatory information irrespective of the modality
in which it is available.

General auditory framework
--------------------------------------------------------------------------------

**General auditory (GA) framework** is the theory that ....

1. Speech sounds are perceived using the same mechanism of audition and
   perceptual learning that have evolved in humans (or human ancestors) to
   handle other classes of environmental sounds

2. Listeners' recovery of spoken messages from the acoustic signal is neither
   equivalent to nor mediated by the perception of gestures; listeners perceive
   the acoustic signal itself rather than speakers vocal tract gestures.
   (contrast MT and DRT)

Listeners perceive speech by combing the acoustic signal for cues that enable
them to activate the mental categories in their particular language that provide
the best match for the signal.

By (2), there are no special mechanisms or modules that explain speech
perception (contrast MT)

Support
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Evidence in support of GA fall into three sets:

1. Evidence that listeners perceive speech and non-speech similarly
   
2. Evidence that (animals can be trained to perform phonetic classification
   tasks with speech stimuli) and (post-training pattern of responses closely
   resembles that of human listeners).
   
3. Evidence that phonemic contrasts tend to lie along natural auditory
   boundaries.

(1) can be interpreted to show that listeners extract statistical patterns in
the acoustic signal, irrespective of the nature of their source, which suggests
that listeners perceive the acoustics of speech much like any other
environmental sounds, and appeals to articulatory process are unnecessary.  (2)
refutes claims of specialization posited for speech perception mechanisms.  (2)
can be interpreted to show that general learning mechanism are responsible for
the specific nature of phonetic categories.  (3) supports the claim that the
phonemic inventories of language are largely determined by consideration of
maximizing auditory distinctiveness. (auditory enhancement hypothesis)

See also
================================================================================

- `Phonetics`
  
- `Cognitive psychology`
  
- `Perception`

Applications
================================================================================

- Computational speech recognition
  
- Foreign-language education

.. [1] http://www.cs.indiana.edu/~port/HDphonol/Diehl.Lotto.Holt.speech.percptn.AnnlRevPsy2003.pdf

---

Introduction ============

When faced with a written word we seem to have no choice to but to read it. See:
Stroop effect. Reading words is such an overlearned skill, it is not easily put
on hold.

Achieving this level of reading skill takes time.

    * A illiterate millionaire, aged 48, required 60 40-hour weeks (14 months)
      of studying to read competently. Probably ballpark of the time required of
      most of us to learn to read.

In addition to being experts in letter recognition, we are especially god at
recognizing them when they spell words.

Our knowledge of spelling and the context provided by the other letters of a
word help us recognize individual letters within words.

Pattern recognition ===================

Approaches to Pattern recognition =================================

Letter recognition ==================

Multifactor Experiments =======================

Models of Recognition =======================

Context Effects in Pattern Recognition ======================================

Artificial Neural Network Models ================================

Justification of Computational Modeling =======================================

Metatheoretical Issues and the Computational Approach
=====================================================


.. [2] Massaro "Models for reading letters and words"

History
================================================================================

Haskins Laboratories was founded in 1935 by Caryl Haskins and Franklin Cooper.

Acoustic analysis of the phonetic elements of speech was made possible by the
development of the sonagraph in the 1950s.

In the 1940s, Alvin Liberman joined Haskins Laboratories to assist in developing
a "sound alphabet" to represent letters in a text for use in a reading machine
for the blind.

In 1950, Cooper competed the Pattern Playback, an early talking device, at
Haskins Laboratories. The machine converts pictures of acoustic patterns in the
form of a spectrogram of speech back into sound.

The pattern-playback device gave scientists control of speech synthesis and
speech waveform.

In the early 1950s, Alvin Liberman, Franklin Cooper, Pierre Delattre, and other
researchers at the Haskins Laboratories carried out a series of landmark studies
on the perceptions of synthetic speech sounds. This work provided the foundation
of what is known about acoustic cues for linguistic units such as phonemes and
features and revealed that the mapping between speech signals and linguistic
units is quite complex.

In 1952, Liberman discovered that the same burst of energy at 1440 Hz before [i]
or [u] is heard as [p], whereas the same burst before [a] is heard as a [k]; the
same bit of the acoustic signal is perceived differently depending on
coarticulatory context.

In 1954, Liberman discovered that very different acoustic cues are heard as the
same sound in syllable context.

In time, Liberman and his collages became convinced that perceived phonemes and
features have a simpler (more nearly one-to-one) relationship to articulation
than to acoustics, and this gave rise to the motor theory of perception.

In the mid 1970s, several new empirical findings posed a challenge to MT, the
then dominant account of human speech perception. Earlier work at Haskins
Laboratories had found clear difference between perception of certain speech
sounds and perception of non-speech analogs of those speech stimuli. Because
these results appears to underscore the special nature of speech perception,
they were interpreted as supporting MT. However, Stevens and several other
research's showed that in some instances perception of speech stimuli does
parallel that of non-speech stimuli provided they share critical temporal
properties.

In 1976, Harry McGurk and John MacDonald published "Hearing Lips and Seeing
Voices" which established the McGurk effect. The effect was discovered by
accident when McGurk and his research assistant, MacDonald, asked a technician
to dub a video with a different phoneme from the one spoken while conducting a
study on how infants perceive language at different developmental stages. When
the video was played back, both researcher heard a third phoneme rather than the
one spoken or mouthed in the video. The McGurk effect demonstrates that
listener's perception of speech is affected vy visual information about vocal
tract movement.

In the 1980s, an alternative to MT, the `direct realist theory`_ of speech
perception, was developed by Carol Fowler also working at the Haskins
Laboratories.

In 1991, Fowler discovered that listeners' perception of speech is affected by
haptic information about vocal tract movement. This reinforces the McGurk
effect.

Research have focused on the mapping between properties of the acoustic signal
and linguistic elements such as phonemes and distinctive features. This mapping
has turned to be quite complex and a complete explanation of how humans
recognize constants and vowels remains elusive.
